{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/econ105/AI/blob/main/bulkimage_caption_blip_by_rd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b949f9f",
      "metadata": {
        "id": "2b949f9f"
      },
      "source": [
        "# BLIP: Bulk Caption Images Automatically\n",
        " - Run each step one after other\n",
        " - Give your Google Drive permission when asked in 1 step\n",
        " - create a folder named \"my_images\" in your Google Drive\n",
        " - Upload images you want to caption in \"my_images\" folder\n",
        " - Image captions will we saved in \"my_captions\" folder in your Google Drive\n",
        " - Caption for each image will be saved as a text file of same name as the    image inside \"my_captions\" folder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Insall Required dependencies"
      ],
      "metadata": {
        "id": "yM1u1-TxEakw"
      },
      "id": "yM1u1-TxEakw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbcb066b",
      "metadata": {
        "id": "cbcb066b"
      },
      "outputs": [],
      "source": [
        "# install requirements\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    print('Running in Colab.')\n",
        "    !pip3 install transformers==4.15.0 timm==0.4.12 fairscale==0.4.4\n",
        "    !git clone https://github.com/salesforce/BLIP\n",
        "    %cd BLIP\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Get images\n",
        "Upload your images to \"my_images\" folder in your Google Drive."
      ],
      "metadata": {
        "id": "jqt4INxYEldw"
      },
      "id": "jqt4INxYEldw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a811a65f",
      "metadata": {
        "id": "a811a65f"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def load_demo_images(image_folder, image_size, device):\n",
        "    images = []\n",
        "    for image_name in os.listdir('/content/drive/MyDrive/my_images/'):\n",
        "        img_path = os.path.join(image_folder, image_name)\n",
        "        raw_image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        w, h = raw_image.size\n",
        "        display(raw_image.resize((w//5, h//5)))\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "        ])\n",
        "        image = transform(raw_image).unsqueeze(0).to(device)\n",
        "        images.append(image)\n",
        "    return images"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f72f4406",
      "metadata": {
        "id": "f72f4406"
      },
      "source": [
        "# 3. Image Captioning\n",
        "- Perform image captioning using finetuned BLIP model.\n",
        "- Result will be saved in \"my_captions\" folder in your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6835daef",
      "metadata": {
        "id": "6835daef"
      },
      "outputs": [],
      "source": [
        "from models.blip import blip_decoder\n",
        "\n",
        "image_size = 512\n",
        "images = load_demo_images('/content/drive/My Drive/my_images', image_size=image_size, device=device)\n",
        "\n",
        "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'\n",
        "model = blip_decoder(pretrained=model_url, image_size=image_size, vit='base')\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "text_folder = '/content/drive/My Drive/my_captions'\n",
        "if not os.path.exists(text_folder):\n",
        "    os.makedirs(text_folder)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, image in enumerate(images):\n",
        "      caption = model.generate(image, sample=False, num_beams=3, max_length=100, min_length=5)\n",
        "      print('caption: '+caption[0])\n",
        "      image_name = os.listdir('/content/drive/My Drive/my_images/')[i]\n",
        "      image_name = os.path.splitext(image_name)[0]\n",
        "      with open(os.path.join(text_folder, f\"{image_name}.txt\"), \"w\") as f:\n",
        "          f.write(caption[0])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "name": "bulkimage_caption_blip_by_rd.ipynb",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}