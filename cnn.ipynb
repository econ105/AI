{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/econ105/AI/blob/main/cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# CIFAR-10 dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "# pretrain data\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohBARUkpO8ri",
        "outputId": "ca619594-706f-4329-f470-dce6a50bffef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 11s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "import numpy as np: This imports the NumPy library and assigns it the alias np. NumPy is a powerful library in Python for numerical computing that provides support for large, multi-dimensional arrays and matrices, as well as a collection of mathematical functions to operate on these arrays.\n",
        "\n",
        "from keras.datasets import cifar10: This imports the CIFAR-10 dataset from the Keras library. CIFAR-10 is a popular benchmark dataset for image classification tasks, consisting of 60,000 32x32 color images in 10 different classes.\n",
        "\n",
        "from keras.utils import to_categorical: This imports the to_categorical function from the Keras library. The to_categorical function is used to convert integer labels into one-hot encoded vectors. One-hot encoding is a common technique used in machine learning where categorical data is represented as binary vectors, with each element in the vector corresponding to a specific category.\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data(): This line of code loads the CIFAR-10 dataset into four variables: train_images (training images), train_labels (training labels), test_images (test images), and test_labels (test labels). The load_data() function returns both the training and test sets.\n",
        "\n",
        "train_images = train_images.astype('float32') / 255 and test_images = test_images.astype('float32') / 255: These lines of code convert the pixel values of the images to floating-point values and normalize them by dividing by 255. This normalization step ensures that the pixel values are in the range of 0 to 1, which is a common practice in image processing and machine learning tasks.\n",
        "\n",
        "train_labels = to_categorical(train_labels) and test_labels = to_categorical(test_labels): These lines of code use the to_categorical function to convert the integer labels (class indices) into one-hot encoded vectors. This is necessary because most machine learning models require the target variables to be in one-hot encoded format for classification tasks.\n",
        "\n",
        "By executing these steps, you would have loaded the CIFAR-10 dataset, preprocessed the images by converting them to floating-point values and normalizing them, and converted the labels to one-hot encoded vectors, making the data suitable for training machine learning models.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "npJ6lizPHHte"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fwo4f15kO1eQ"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Set up CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from keras.models import Sequential and from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense: These import statements bring in the necessary classes from Keras to define and build the CNN model. Sequential is a Keras class that allows us to build a model layer by layer, and the other classes (Conv2D, MaxPooling2D, Flatten, Dense) are different types of layers that can be added to the model.\n",
        "\n",
        "model = Sequential([...]): This initializes a new sequential model. The model will be built by adding layers one after another in the specified order.\n",
        "\n",
        "Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)): This line adds a 2D convolutional layer to the model. The first argument, 32, represents the number of filters (also known as channels) in the layer. The second argument, (3, 3), defines the size of the filters. The activation='relu' parameter specifies that the rectified linear unit (ReLU) activation function will be used for this layer. The input_shape=(32, 32, 3) parameter defines the shape of the input data, which is a 32x32 image with 3 color channels (RGB).\n",
        "\n",
        "MaxPooling2D((2, 2)): This line adds a 2D max pooling layer to the model. The (2, 2) parameter specifies the size of the pooling window. Max pooling is a downsampling operation that reduces the spatial dimensions of the input by taking the maximum value within each pooling window.\n",
        "\n",
        "The next three lines (Conv2D, MaxPooling2D, Conv2D) add additional convolutional and max pooling layers to the model, following a similar pattern as described in step 3.\n",
        "\n",
        "Flatten(): This line adds a flatten layer to the model. The flatten layer is used to convert the multi-dimensional output from the previous layer into a one-dimensional vector, which can be passed to a fully connected (dense) layer.\n",
        "\n",
        "Dense(64, activation='relu'): This line adds a fully connected (dense) layer to the model. The first argument, 64, specifies the number of neurons in the layer. The activation='relu' parameter specifies the activation function for this layer.\n",
        "\n",
        "Dense(10, activation='softmax'): This line adds the final dense layer to the model. The first argument, 10, specifies the number of neurons in the layer, which corresponds to the number of classes in the classification task. The activation='softmax' parameter specifies the activation function for this layer, which is softmax. Softmax activation is commonly used in multiclass classification problems to produce class probabilities.\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']): This line compiles the model. The optimizer='adam' parameter specifies the optimization algorithm to be used during training, which is Adam in this case. The loss='categorical_crossentropy' parameter specifies the loss function to be optimized, which is the categorical cross-entropy. The metrics=['accuracy'] parameter specifies that the accuracy metric will be computed during training and evaluation."
      ],
      "metadata": {
        "id": "4mZFbafaHW7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_images, train_labels, epochs=10, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WowyYM9OPJQa",
        "outputId": "be4513d9-95df-44d8-ad08-269266c57baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 60s 75ms/step - loss: 1.6056 - accuracy: 0.4143\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 59s 75ms/step - loss: 1.2264 - accuracy: 0.5619\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 59s 76ms/step - loss: 1.0628 - accuracy: 0.6264\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 66s 85ms/step - loss: 0.9583 - accuracy: 0.6633\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 62s 79ms/step - loss: 0.8855 - accuracy: 0.6885\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 58s 74ms/step - loss: 0.8294 - accuracy: 0.7095\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 58s 75ms/step - loss: 0.7820 - accuracy: 0.7249\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 57s 73ms/step - loss: 0.7430 - accuracy: 0.7389\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 58s 74ms/step - loss: 0.6965 - accuracy: 0.7561\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 57s 73ms/step - loss: 0.6653 - accuracy: 0.7671\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e25205254b0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_images: This is the input training data, which consists of a set of images. The shape of train_images should be (num_samples, height, width, num_channels), where num_samples is the number of training samples, height and width are the dimensions of the images, and num_channels is the number of color channels (e.g., 3 for RGB images).\n",
        "\n",
        "train_labels: This is the target training labels corresponding to the train_images. The shape of train_labels should be (num_samples, num_classes), where num_classes is the number of classes in the classification task. The labels should be in one-hot encoded format.\n",
        "\n",
        "epochs=10: This parameter specifies the number of times the model will iterate over the entire training dataset. In this case, the model will go through the training dataset 10 times.\n",
        "\n",
        "batch_size=64: This parameter determines the number of samples per gradient update. During training, the model processes the data in batches instead of using the entire dataset at once. The batch size of 64 means that the model will update its weights and biases after processing 64 training samples.\n",
        "\n",
        "When you call model.fit(), the training process begins. The model will iterate over the training dataset for the specified number of epochs, processing the data in batches. During each epoch, the model will compute the gradients, update the weights and biases based on the loss function and optimizer specified during model compilation, and track the training accuracy.\n",
        "\n",
        "The fit() function executes the training process and returns a history object that contains information about the training process, such as the loss and accuracy values at each epoch. This object can be used for further analysis or visualization of the training results.\n",
        "\n",
        "After training is complete, the model will have learned to make predictions on new, unseen data based on the patterns it learned from the training dataset."
      ],
      "metadata": {
        "id": "fbqtX1yuHgcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2hbPt-APTiU",
        "outputId": "f7734e4d-a5fb-4ee1-bdfe-f84d89802e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 4s 12ms/step - loss: 0.8250 - accuracy: 0.7216\n",
            "Test accuracy: 0.7215999960899353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "test_images: This is the input test data, which consists of a set of images. The shape of test_images should be the same as the training data (num_samples, height, width, num_channels).\n",
        "\n",
        "test_labels: This is the target test labels corresponding to the test_images. The shape of test_labels should also be the same as the training labels (num_samples, num_classes).\n",
        "\n",
        "When you call model.evaluate(), the trained model will make predictions on the test data and compute the loss and accuracy metrics based on the predictions and the provided ground truth labels.\n",
        "\n",
        "The code assigns the values of the computed test loss and accuracy to the variables test_loss and test_acc, respectively. These variables will hold the results of the evaluation.\n",
        "\n",
        "Finally, print('Test accuracy:', test_acc) is used to display the test accuracy metric, which provides an indication of how well the model performs on unseen data. The test accuracy is calculated as the ratio of correctly classified samples to the total number of test samples.\n",
        "\n",
        "By evaluating the model on the test data, you can assess its generalization performance and determine how well it performs on data that it hasn't seen during training. This step is crucial to understand the model's performance and its ability to make accurate predictions on real-world, unseen data."
      ],
      "metadata": {
        "id": "UI4hMkwhHovl"
      }
    }
  ]
}