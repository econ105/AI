{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/econ105/AI/blob/main/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QckaKzPetL54",
        "outputId": "1c5278b6-90d7-4f0c-ef1c-8d7fd8c5024a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The command \"!pip install textblob\" is used in a Python environment, such as Google Colab, to install the TextBlob library.\n",
        "\n",
        "TextBlob is a popular Python library that provides a simple and intuitive API for common natural language processing (NLP) tasks. It is built on top of other libraries, including NLTK (Natural Language Toolkit) and Pattern, and provides an easy-to-use interface for tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, and more.\n",
        "\n",
        "When you run the \"!pip install textblob\" command in a Python environment, it instructs the package manager \"pip\" to download and install the TextBlob library and its dependencies. Pip is the default package manager for Python and is used to install and manage external libraries and packages.\n",
        "\n",
        "After executing the command, you should be able to import the TextBlob library in your Python code and use its functionalities. For example, you can perform sentiment analysis on a text, extract key phrases, or perform various other NLP tasks provided by TextBlob."
      ],
      "metadata": {
        "id": "jESwomF9DVf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m textblob.download_corpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn0lAHddtMrz",
        "outputId": "b6b93684-40a2-48f9-84a6-dfaa70553592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The command \"!python -m textblob.download_corpora\" is used to download the required corpora for the TextBlob library in a Python environment.\n",
        "\n",
        "Corpora in TextBlob refer to large collections of text data that are used for various NLP tasks, such as training language models, performing sentiment analysis, and more. The TextBlob library relies on specific corpora for certain functionalities to work properly.\n",
        "\n",
        "When you run the \"!python -m textblob.download_corpora\" command, it executes a Python module called \"textblob\" with the argument \"download_corpora\". This module is responsible for managing the corpora used by TextBlob.\n",
        "\n",
        "By running this command, the module will connect to the TextBlob server and download the necessary corpora files to your system. These corpora files include datasets for tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, word frequencies, and more.\n",
        "\n",
        "Downloading the corpora is important because it allows TextBlob to perform accurate and reliable NLP tasks. Without the corpora, certain features and functionalities of TextBlob may not work as expected.\n",
        "\n",
        "It's worth noting that the download process may take some time depending on your internet connection speed and the size of the corpora being downloaded. Once the download is complete, you should be able to utilize the full range of TextBlob features that rely on the downloaded corpora in your Python code."
      ],
      "metadata": {
        "id": "Pdd20XzyDgn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = \"Today is a beautiful day\"\n",
        "blob = TextBlob(text)\n",
        "\n",
        "blob.words # Word tokenization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPvB6zBJtQ37",
        "outputId": "21c23867-a8b8-4808-f891-7f4a6c9f9828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['Today', 'is', 'a', 'beautiful', 'day'])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet you provided demonstrates the usage of the TextBlob library in Python to perform word tokenization.\n",
        "\n",
        "First, the line \"from textblob import TextBlob\" imports the TextBlob class from the textblob module. This allows us to create instances of the TextBlob class and use its functionalities.\n",
        "\n",
        "Next, the variable \"text\" is assigned the string \"Today is a beautiful day\". This is the text that we want to tokenize into individual words.\n",
        "\n",
        "The line \"blob = TextBlob(text)\" creates a TextBlob object called \"blob\" by passing the \"text\" variable as an argument. The TextBlob object represents the text and provides various methods and properties for text analysis.\n",
        "\n",
        "Finally, the line \"blob.words\" demonstrates word tokenization. The \"words\" property of the TextBlob object returns a WordList object, which is essentially a list-like object containing the individual words extracted from the text. In this case, it will return ['Today', 'is', 'a', 'beautiful', 'day'], which are the individual words in the provided text.\n",
        "\n",
        "Word tokenization is the process of splitting a text into individual words or tokens. It is a common preprocessing step in natural language processing tasks. The TextBlob library provides convenient methods like \"words\" to perform tokenization, making it easy to work with text data in a Python environment."
      ],
      "metadata": {
        "id": "pV4TeIB5FCQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blob.noun_phrases # Noun phrase extraction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMUrDTkStUVE",
        "outputId": "5a7393e2-6b2a-4af5-fb6b-3b459c906eac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['beautiful day'])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blob.sentiment # Sentiment analysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuPpBEyvtYNv",
        "outputId": "2924680c-c77e-4890-d1cf-4b81108836af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=0.85, subjectivity=1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blob.word_counts # Word counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqQSrwObtace",
        "outputId": "35df936a-5088-467c-8fbd-f40b95992ebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {'today': 1, 'is': 1, 'a': 1, 'beautiful': 1, 'day': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spelling correction\n",
        "text = \"Today is a beutiful day\"\n",
        "blob = TextBlob(text)\n",
        "blob.correct()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKT7lGsMtbuK",
        "outputId": "5f8c0d38-4758-4bc0-dd11-ca6b392a4e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"Today is a beautiful day\")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlxtend # Convert names into a generalized format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDs0kiqxtej7",
        "outputId": "a0ae82e8-37b2-4f6f-ce80-edcc5f06dbbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.11.4)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (3.7.1)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from mlxtend) (67.7.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->mlxtend) (2023.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->mlxtend) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.text import generalize_names\n",
        "\n",
        "generalize_names(\"Tran, Khuyen\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EbijaeOAtkOR",
        "outputId": "0ddc94f4-9a03-4400-d6f4-5c520a85b556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tran k'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These lines two demonstrates the usage of the generalize_names function from the mlxtend.text module in Python.\n",
        "\n",
        "First, the line \"from mlxtend.text import generalize_names\" imports the generalize_names function from the mlxtend.text module. This allows us to use the function in our code.\n",
        "\n",
        "Next, the line generalize_names(\"Tran, Khuyen\") calls the generalize_names function with the string \"Tran, Khuyen\" as an argument. The purpose of this function is to generalize or anonymize personal names by replacing them with more generic labels.\n",
        "\n",
        "The generalize_names function is a part of the mlxtend library, which provides various utilities and extensions for machine learning tasks. In this case, the generalize_names function specifically focuses on anonymizing personal names.\n",
        "\n",
        "By passing \"Tran, Khuyen\" as the argument to generalize_names, the function will process the input name and return a generalized version. The exact behavior and output of the function depend on the implementation and configuration of the generalize_names function in the mlxtend library.\n",
        "\n",
        "In summary, the code snippet utilizes the generalize_names function from the mlxtend.text module to anonymize the personal name \"Tran, Khuyen\". The function applies a generalization process to replace the specific name with a more generic label."
      ],
      "metadata": {
        "id": "BVFEuhFUF0Fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generalize_names(\"Khuyen Tran\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZPLiu4mGtp7F",
        "outputId": "e0542ed1-204d-408f-8edd-f1632102ebe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tran k'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case of generalize_names(\"Khuyen Tran\"), the function would take the input name \"Khuyen Tran\" and potentially transform it into a more generic label. The specific output of the function would depend on the implementation and configuration of the generalize_names function within the mlxtend library."
      ],
      "metadata": {
        "id": "QmRb--5KGQ_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generalize_names(\"Khuyen Tran\", firstname_output_letters=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cLhw3ej9tr34",
        "outputId": "b8c45508-cd08-48a8-90ab-1ea83076807f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tran kh'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet `generalize_names(\"Khuyen TranI apologize for the incomplete response.\n",
        "\n",
        "The additional argument firstname_output_letters=2 in the generalize_names(\"Khuyen Tran\", firstname_output_letters=2) function call suggests that the generalize_names function allows for customization of the output for the first name by specifying the number of output letters.\n",
        "\n",
        "Based on this argument, it appears that the generalize_names function can potentially truncate the first name to only include the first two letters. So, in this case, the output for the first name \"Khuyen\" would be \"Kh\".\n",
        "\n",
        "It's important to note that the exact behavior and output of the generalize_names function depend on its specific implementation within the mlxtend.text module. The function may offer various customization options to generalize or anonymize names, and the firstname_output_letters argument is just one example.\n",
        "\n",
        "To fully understand the functionality and behavior of the generalize_names function, it is recommended to refer to the documentation or specific implementation details of the mlxtend.text library or consult the source code if available."
      ],
      "metadata": {
        "id": "bP900t9BGhaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy # Sunnarize text in one line of code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q6gdeLut0YA",
        "outputId": "35dfd7bc-b68d-4e81-f13e-6a4a48eb4cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/97.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.31.0)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-23.12.11-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.66.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2024.2.2)\n",
            "Building wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21691 sha256=03849309f3ad998d6c0891d63e9870d68ebaa792579508eb009ce9d2dd083379\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=38ba937bdc6358a47ae82730e94408cf274329f2630554b6648b8f8c1b417961\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-23.12.11 sumy-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The command \"!pip install sumy\" is used to install the Sumy library in a Python environment.\n",
        "\n",
        "Sumy is a Python library that provides a simple and convenient way to summarize text documents. It utilizes various algorithms and approaches to extract the most important sentences or phrases from a given text and generate a concise summary.\n",
        "\n",
        "Once you have installed Sumy using the \"!pip install sumy\" command, you can summarize a text document in just one line of code. Here's an example:"
      ],
      "metadata": {
        "id": "V6pi0aZJJu9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sumy lex-rank --length=10 --url=https://www.dataquest.io/blog/learn-data-science/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icY92GcEt3ui",
        "outputId": "38d5be3f-728a-4ed3-8761-71703f4aa636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skill Path\n",
            "Learn how to analyze data using Python, one of the most popular programming languages, and kick-start your data career.\n",
            "You’ll learn the fundamentals of Python to prepare, explore, and analyze data.\n",
            "skill Path\n",
            "You’ll learn the fundamentals of Python to prepare, explore, analyze, and build data visualizations.\n",
            "Learn data cleaning, one of the most crucial skills you need in your data career.\n",
            "You’ll learn the basic statistical analysis and probability techniques as well as the fundamentals of Python.\n",
            "You’ll learn how to collect your own data from APIs and the web using R, and you’ll start data projects.\n",
            "You’ll learn the fundamentals of R to prepare, explore, and analyze data.\n",
            "Learn fundamental concepts of machine learning, apply an array of machine learning algorithms, implement techniques to build, test, train, and optimize your models, and make accurate data-driven predictions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The command \"!sumy lex-rank --length=10 --url=https://www.dataquest.io/blog/learn-data-science/\" is a command-line instruction that utilizes the Sumy library's command-line interface to summarize a web page using the LexRank algorithm.\n",
        "\n",
        "Here's an explanation of the different components of the command:\n",
        "\n",
        "    !sumy: This is the command to execute the Sumy library's command-line interface.\n",
        "\n",
        "    lex-rank: Specifies the algorithm to be used for summarization. In this case, LexRank is chosen as the algorithm. LexRank is a graph-based algorithm that ranks sentences based on their similarities to identify the most important sentences for the summary.\n",
        "\n",
        "    --length=10: Specifies the desired length of the summary in terms of the number of sentences. In this case, it is set to 10, meaning the summary will contain 10 sentences.\n",
        "\n",
        "    --url=https://www.dataquest.io/blog/learn-data-science/: Specifies the URL of the web page that you want to summarize. In this example, the web page at \"https://www.dataquest.io/blog/learn-data-science/\" will be summarized.\n",
        "\n",
        "When you execute this command, Sumy will scrape the web page content from the provided URL, apply the LexRank summarization algorithm, and generate a summary consisting of 10 sentences. The resulting summary will be displayed in the command-line output."
      ],
      "metadata": {
        "id": "m9QSlI_QKaWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy textacy # Extract a contiguous sequence of words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7Hls5WuwO9C",
        "outputId": "19f36bd9-a481-457d-e776-204f36869796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: textacy in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (5.3.3)\n",
            "Requirement already satisfied: cytoolz>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from textacy) (0.12.3)\n",
            "Requirement already satisfied: floret~=0.10.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (0.10.5)\n",
            "Requirement already satisfied: jellyfish>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.0.3)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.3.2)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from textacy) (3.2.1)\n",
            "Requirement already satisfied: pyphen>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (0.14.0)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.2.2)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cytoolz>=0.10.1->textacy) (0.12.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->textacy) (3.3.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The command \"!pip install spacy textacy\" is used to install two Python libraries: Spacy and Textacy.\n",
        "\n",
        "Spacy is a popular library for natural language processing (NLP) tasks. It provides efficient and easy-to-use tools for various NLP tasks, including tokenization, part-of-speech tagging, named entity recognition, and more.\n",
        "\n",
        "Textacy is a higher-level library built on top of Spacy that simplifies common text processing tasks. It offers additional functionalities and utilities to work with text data, such as text preprocessing, n-gram extraction, topic modeling, and text summarization.\n",
        "\n",
        "Once you have installed Spacy and Textacy using the \"!pip install spacy textacy\" command, you can utilize their functionalities in your Python code.\n"
      ],
      "metadata": {
        "id": "4-14WktcKmY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Lebp1EVwPD8",
        "outputId": "ab013ed3-9de9-4d97-9b91-230aebb5e298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The command !python -m spacy download en_core_web_sm is used to download and install the English language model \"en_core_web_sm\" for the Spacy library.\n",
        "\n",
        "Spacy is a popular open-source library for natural language processing (NLP) tasks in Python. It offers various pre-trained language models that enable you to perform tasks such as tokenization, part-of-speech tagging, named entity recognition, and dependency parsing.\n",
        "\n",
        "The \"en_core_web_sm\" model is one of the pre-trained English language models provided by Spacy. It is a small-sized model that includes vocabulary, syntax, and word vectors trained on a large corpus of English text. The \"sm\" in \"en_core_web_sm\" stands for \"small.\"\n",
        "\n",
        "By installing the \"en_core_web_sm\" model, you gain access to pre-trained linguistic annotations and models specifically designed for English text processing in Spacy.\n",
        "\n",
        "The !python -m spacy download en_core_web_sm command is executed in the command-line interface (CLI) or terminal. It leverages the Python interpreter to run the spacy download command with the argument en_core_web_sm. This triggers the installation process, and the necessary files for the \"en_core_web_sm\" model are downloaded and installed into your Spacy library.\n",
        "\n",
        "Once the installation is complete, you can load the \"en_core_web_sm\" model in your Python code using the spacy.load('en_core_web_sm') command. This allows you to use the linguistic annotations and models provided by the \"en_core_web_sm\" model for various NLP tasks in Spacy.\n",
        "\n",
        "It's important to note that the exclamation mark (!) at the beginning of the command indicates that it is executed in a shell or command-line environment, rather than being a pure Python command."
      ],
      "metadata": {
        "id": "MOKP13p2Lbq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from textacy.extract import ngrams\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = nlp(\"Ice cream is a soft frozen food made with sweetened and flavored milk fat.\")\n",
        "\n",
        "# extract sequences of 3 words\n",
        "[n.text for n in ngrams(text, n=3)]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WLo2_qiwPKy",
        "outputId": "5f45a8b0-846c-491c-86c6-59ae73c41901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['soft frozen food', 'sweetened and flavored', 'flavored milk fat']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet you provided demonstrates how to use the Spacy library and the textacy.extract.ngrams function to extract n-grams from a given text.\n",
        "\n",
        "Here's a breakdown of the code:\n",
        "\n",
        "    import spacy: This imports the Spacy library, which is a popular Python library for natural language processing tasks.\n",
        "\n",
        "    from textacy.extract import ngrams: This imports the ngrams function from the textacy.extract module. ngrams is a function provided by the Textacy library, which is built on top of Spacy and offers additional text processing utilities.\n",
        "\n",
        "    nlp = spacy.load(\"en_core_web_sm\"): This loads the English language model \"en_core_web_sm\" from Spacy. The model provides linguistic annotations and capabilities for English text processing.\n",
        "\n",
        "    text = nlp(\"Ice cream is a soft frozen food made with sweetened and flavored milk fat.\"): This processes the input text using the Spacy language model. The text is tokenized, and each token is assigned linguistic annotations like part-of-speech tags, dependency relationships, etc.\n",
        "\n",
        "    [n.text for n in ngrams(text, n=3)]: This line extracts n-grams from the processed text. The ngrams function takes two parameters: the processed text (text) and the value of n which specifies the length of the desired n-grams. In this example, n-grams of length 3 are extracted. The resulting n-grams are then converted into a list of their text representations using a list comprehension.\n",
        "\n",
        "The output of this code would be a list of text representations of the extracted n-grams, where each n-gram consists of a sequence of three words. For example, the output might look like ['Ice cream is', 'cream is a', 'is a soft', 'a soft frozen', 'soft frozen food', 'frozen food made', 'food made with', 'made with sweetened', 'with sweetened and', 'sweetened and flavored', 'and flavored milk', 'flavored milk fat'].\n",
        "\n",
        "By using this code, you can extract n-grams of a specified length from a given text using Spacy and Textacy."
      ],
      "metadata": {
        "id": "LxhRYpllLgMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordfreq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AJzw_YBwPUe",
        "outputId": "331c9ff4-4db9-4d8f-e040-6bd7b20fdd91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wordfreq\n",
            "  Downloading wordfreq-3.1.1-py3-none-any.whl (56.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy>=6.1 (from wordfreq)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.10/dist-packages (from wordfreq) (3.3.0)\n",
            "Collecting locate<2.0.0,>=1.1.1 (from wordfreq)\n",
            "  Downloading locate-1.1.1-py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from wordfreq) (1.0.8)\n",
            "Requirement already satisfied: regex>=2023.10.3 in /usr/local/lib/python3.10/dist-packages (from wordfreq) (2023.12.25)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1->wordfreq) (0.2.13)\n",
            "Installing collected packages: locate, ftfy, wordfreq\n",
            "Successfully installed ftfy-6.1.3 locate-1.1.1 wordfreq-3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The command \"!pip install wordfreq\" is used to install the WordFreq library in a Python environment.\n",
        "\n",
        "WordFreq is a Python library that provides access to word frequency data in multiple languages. It allows you to retrieve information about the frequency of words in a given language based on large-scale text corpora.\n",
        "\n",
        "Once you have installed WordFreq using the \"!pip install wordfreq\" command, you can utilize its functionalities in your Python code.\n",
        "\n",
        "Some of the common use cases of WordFreq include:\n",
        "\n",
        "    Word Frequency Lookup: You can use WordFreq to retrieve the frequency of a specific word in a given language. This can be useful for various text analysis tasks or for building language-related applications.\n",
        "\n",
        "    Rank Words by Frequency: WordFreq provides methods to rank words by their frequency in a specific language. You can obtain a list of the most common words or statistically significant words in a given text corpus.\n",
        "\n",
        "    Frequency Distribution: WordFreq allows you to generate frequency distributions of words in a text corpus, providing insights into the overall word usage patterns.\n"
      ],
      "metadata": {
        "id": "KbiSIiq5OGCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordfreq import word_frequency\n",
        "word_frequency(\"eat\", \"en\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JN87W_DwPXK",
        "outputId": "9d3f4f33-fd06-4956-e927-6e3d16d709e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.000135"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet you provided demonstrates how to import the necessary libraries and use the word_frequency function from the WordFreq library to calculate the frequency of a specific word in English. Here's a breakdown of the code:\n",
        "\n",
        "    import matplotlib.pyplot as plt: This line imports the pyplot module from the matplotlib library and assigns it the alias plt. pyplot provides a collection of functions that allow you to create various types of plots and visualizations.\n",
        "\n",
        "    import seaborn as sns: This line imports the seaborn library, which is a data visualization library built on top of matplotlib. Seaborn provides a high-level interface for creating attractive and informative statistical graphics.\n",
        "\n",
        "    from wordfreq import word_frequency: This line imports the word_frequency function from the WordFreq library. The word_frequency function is used to calculate the frequency of a word in a specific language based on large-scale text corpora.\n",
        "\n",
        "    word_frequency(\"eat\", \"en\"): This line calls the word_frequency function to calculate the frequency of the word \"eat\" in English. The first argument is the word for which you want to calculate the frequency, and the second argument specifies the language, in this case, \"en\" for English.\n"
      ],
      "metadata": {
        "id": "mJvgMRH7PSaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"There is a dog running in a park\"\n",
        "words = sentence.split(\" \")\n",
        "word_frequencies = [word_frequency(word, \"en\") for word in words]\n",
        "\n",
        "sns.barplot( word_frequencies)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "C2KhHI6CwPas",
        "outputId": "f0e4b6b0-3766-475b-c931-aaea474100a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet you provided demonstrates how to calculate word frequencies for a given sentence and visualize them using a bar plot using the word_frequency function and the sns.barplot function from the seaborn library. Here's a breakdown of the code:\n",
        "\n",
        "    sentence = \"There is a dog running in a park\": This line defines a string variable sentence that contains the input sentence for which you want to calculate word frequencies.\n",
        "\n",
        "    words = sentence.split(\" \"): This line splits the sentence into individual words and creates a list called words where each element represents a word from the sentence. The split(\" \") method is used to split the sentence at each space character, treating spaces as word separators.\n",
        "\n",
        "    word_frequencies = [word_frequency(word, \"en\") for word in words]: This line calculates the word frequencies for each word in the words list. The word_frequency function is used to get the frequency of each word. The second argument, \"en\", specifies that you want to calculate the frequencies for English words. The resulting frequencies are stored in the word_frequencies list using a list comprehension.\n",
        "\n",
        "    sns.barplot(word_frequencies): This line creates a bar plot using the sns.barplot function from the seaborn library. The word_frequencies list is passed as the data for the plot, and seaborn automatically calculates the count or frequency of each unique value in the list and displays it as a bar.\n",
        "\n",
        "    plt.show(): This line displays the bar plot on the screen.\n"
      ],
      "metadata": {
        "id": "gt24ebWVPH-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers\n",
        "from transformers import pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "data = [\"I love you\", \"I hate you\"]\n",
        "sentiment_pipeline(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N29oNUg19m3",
        "outputId": "9f504730-89c9-40f9-bc76-4bca00163075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9998656511306763},\n",
              " {'label': 'NEGATIVE', 'score': 0.9991129040718079}]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet you provided demonstrates how to install the Transformers library and use it to perform sentiment analysis using a pre-trained model. Here's a breakdown of the code:\n",
        "\n",
        "    !pip install -q transformers: This line installs the Transformers library using pip. The -q flag is used to suppress output during installation.\n",
        "\n",
        "    from transformers import pipeline: This line imports the pipeline class from the Transformers library. The pipeline class provides a high-level API for performing various natural language processing tasks, including sentiment analysis.\n",
        "\n",
        "    sentiment_pipeline = pipeline(\"sentiment-analysis\"): This line creates a sentiment analysis pipeline using the pipeline class. The \"sentiment-analysis\" argument specifies that we want to use a pre-trained sentiment analysis model.\n",
        "\n",
        "    data = [\"I love you\", \"I hate you\"]: This line defines a list called data that contains two sentences: \"I love you\" and \"I hate you\". These are the input sentences for which we want to perform sentiment analysis.\n",
        "\n",
        "    sentiment_pipeline(data): This line calls the sentiment_pipeline with the data list as input. The pipeline processes each sentence using the pre-trained sentiment analysis model and returns the predicted sentiment for each sentence.\n",
        "\n",
        "The output of this code would be a list of dictionaries, where each dictionary contains the text of the sentence and its corresponding sentiment label. For example, the output might look like [{'label': 'POSITIVE', 'score': 0.9998766183853149}, {'label': 'NEGATIVE', 'score': 0.9998604655265808}]. The 'label' key indicates the sentiment label (e.g., 'POSITIVE' or 'NEGATIVE'), and the 'score' key represents the confidence or probability associated with the sentiment prediction."
      ],
      "metadata": {
        "id": "dZb1gEaYPitY"
      }
    }
  ]
}