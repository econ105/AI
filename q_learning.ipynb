{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/econ105/AI/blob/main/q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Q table\n",
        "q = np.zeros((6, 6))\n",
        "q = np.matrix(q)\n",
        "\n",
        "# R table\n",
        "r = np.array([[-1, -1, -1, -1, 0, -1], [-1, -1, -1, 0, -1, 100], [-1, -1, -1, 0, -1, -1], [-1, 0, 0, -1, 0, -1],\n",
        "              [0, -1, -1, 0, -1, 100], [-1, 0, -1, -1, 0, 100]])\n",
        "r = np.matrix(r)\n",
        "\n",
        "# discount factor\n",
        "gamma = 0.8"
      ],
      "metadata": {
        "id": "oCIqnHV1FUkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code imports two libraries: NumPy (np) and random.\n",
        "\n",
        "The first section of code creates a table called \"q\" using NumPy's array function. It initializes all elements of the 6x6 table to zero. Then, it converts the array to a matrix using NumPy's matrix function.\n",
        "\n",
        "The second section of code creates another table called \"r\" using NumPy's array function. It initializes each element of the 6x6 table with values from the list [-1, -1, -1, -1, 0, -1], [-1, -1, -1, 0, -1, 100], [-1, -1, -1, 0, -1, -1], [-1, 0, 0, -1, 0, -1], [0, -1, -1, 0, -1, 100], and [-1, 0, -1, -1, 0, 100]. It then converts the array to a matrix using NumPy's matrix function.\n",
        "\n",
        "The third section of code sets a variable called \"gamma\" to the value 0.8. This variable is likely used as a discount factor in a reinforcement learning algorithm.\n",
        "\n",
        "In reinforcement learning, a discount factor is a parameter that determines how much the algorithm values future rewards. It is a way to reduce the importance of future rewards relative to immediate rewards.\n",
        "\n",
        "In reinforcement learning, an agent receives rewards for its actions over time. The goal of the agent is to maximize the cumulative reward over time. However, the agent may face a trade-off between receiving a smaller reward now or a larger reward later. The discount factor determines how much the agent values future rewards relative to immediate rewards.\n",
        "\n",
        "A discount factor of 0 means that the agent values immediate rewards and does not consider future rewards. A discount factor close to 1 means that the agent values future rewards almost as much as immediate rewards. A discount factor of 1 means that the agent values future rewards equally to immediate rewards.\n",
        "\n",
        "The discount factor is used in Q-learning and other reinforcement learning algorithms to update the Q-table. When updating the Q-table, the algorithm discounts the future rewards by the discount factor, which means that the algorithm values immediate rewards more than future rewards.\n",
        "\n",
        "For example, suppose an agent receives a reward of 100 now and expects to receive a reward of 150 in the future. If the discount factor is 0.8, the agent values the future reward of 150 as 120 (150 x 0.8). The agent will choose the action that leads to the higher immediate reward, even if it means forgoing the future reward.\n",
        "\n",
        "The discount factor is a hyperparameter that the algorithm sets before training. It is a trade-off between exploration and exploitation. A high discount factor means that the agent will exploit the current knowledge and receive immediate rewards. A low discount factor means that the agent will explore and delay receiving rewards to maximize the cumulative reward over time.\n",
        "\n",
        "In summary, a discount factor in reinforcement learning is a parameter that determines how much the algorithm values future rewards. It is a way to reduce the importance of future rewards relative to immediate rewards. The discount factor is used in Q-learning and other reinforcement learning algorithms to update the Q-table and determine the agent's policy."
      ],
      "metadata": {
        "id": "mER-mdozQSJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing an appropriate discount factor for reinforcement learning algorithms can be challenging, as it depends on various factors, such as the problem domain, the desired level of exploration, and the agent's objective. However, there are some guidelines and best practices that can help in selecting a suitable discount factor:\n",
        "\n",
        "    Problem Domain: The discount factor should reflect the temporal structure of the problem domain. For example, in problems with long-term dependencies, a smaller discount factor may be more appropriate to capture the long-term effects of actions. In contrast, a larger discount factor may be suitable for problems with short-term dependencies.\n",
        "    Exploration-Exploitation Trade-off: The discount factor controls the exploration-exploitation trade-off. A smaller discount factor encourages the agent to explore more, while a larger discount factor encourages the agent to exploit the current knowledge. The choice of the discount factor depends on the desired level of exploration versus exploitation.\n",
        "    Reward Structure: The discount factor should be chosen based on the structure of the reward function. If the reward function has a high-frequency component, a smaller discount factor may be more appropriate to capture the short-term dependencies. If the reward function has a low-frequency component, a larger discount factor may be more appropriate to capture the long-term dependencies.\n",
        "    Decay Rate: The discount factor should decay over time to encourage the agent to explore new actions and states. A common practice is to use an exponential decay rate, where the discount factor decreases exponentially over time.\n",
        "    Empirical Methods: Empirical methods, such as grid search or random search, can be used to find the optimal discount factor for a given problem. These methods involve testing different discount factors and evaluating the performance of the agent.\n",
        "    Domain Knowledge: Domain knowledge can be used to choose an appropriate discount factor. For example, in problems where the agent needs to learn long-term dependencies, a smaller discount factor may be more appropriate. In contrast, a larger discount factor may be suitable for problems where the agent needs to adapt quickly to changing conditions.\n",
        "    Hyperparameter Tuning: The discount factor is a hyperparameter that can be tuned using various methods, such as grid search, random search, or Bayesian optimization. These methods involve testing different values of the discount factor and evaluating the performance of the agent.\n",
        "    Agent's Objective: The discount factor should align with the agent's objective. For example, if the agent's objective is to maximize the cumulative reward over a short time horizon, a larger discount factor may be more appropriate. If the agent's objective is to maximize the cumulative reward over a long time horizon, a smaller discount factor may be more appropriate.\n"
      ],
      "metadata": {
        "id": "LJ0ratatQx33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000):\n",
        "    # radomly select\n",
        "    state = random.randint(0, 5)\n",
        "    while state != 5:\n",
        "        r_pos_action = []\n",
        "        for action in range(6):\n",
        "            if r[state, action] >= 0:\n",
        "                r_pos_action.append(action)\n",
        "        next_state = r_pos_action[random.randint(0, len(r_pos_action) - 1)]\n",
        "        q[state, next_state] = r[state, next_state] + gamma * q[next_state].max()\n",
        "        state = next_state"
      ],
      "metadata": {
        "id": "jt6S0luiKkTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "    for i in range(1000):: This line loops the code inside it 1000 times.\n",
        "    state = random.randint(0, 5): This line selects a random state from 0 to 5.\n",
        "    while state != 5: This line starts a while loop that runs until the state is 5.\n",
        "    r_pos_action = []: This line initializes an empty list called r_pos_action that will store the possible actions for the current state.\n",
        "    for action in range(6):: This line loops through all possible actions (0 to 5) for the current state.\n",
        "    if r[state, action] >= 0:: This line checks if the reward for the current state and action is non-negative. If it is, the action is added to the r_pos_action list.\n",
        "    next_state = r_pos_action[random.randint(0, len(r_pos_action) - 1)]: This line selects a random action from the r_pos_action list and sets it as the next state.\n",
        "    q[state, next_state] = r[state, next_state] + gamma * q[next_state].max(): This line updates the Q-value for the current state and next state. The Q-value is the sum of the immediate reward and the discounted maximum Q-value of the next state. The discount factor gamma is used to determine how much the Q-value of the next state is worth compared to the immediate reward.\n",
        "    state = next_state: This line sets the current state to the next state.\n",
        "\n",
        "The code inside the loop runs until the state is 5, which means the agent has reached the final state. The Q-values are updated at each step, and the agent learns the optimal policy by exploring the environment.\n",
        "\n",
        "The purpose of Q-learning is to learn the optimal policy for a given environment. The agent learns by exploring the environment, and the Q-values are updated based on the rewards received. The Q-values represent the expected return for taking a particular action in a particular state. The agent uses the Q-values to determine the best action to take in a given state, which maximizes the expected cumulative reward.\n"
      ],
      "metadata": {
        "id": "zY_7OGbNQ7GK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = random.randint(0, 5)\n",
        "print('the robot goes to{}'.format(state))\n",
        "count = 0\n",
        "while state != 5:\n",
        "    if count > 20:   # if>20, fail\n",
        "        print('fail')\n",
        "        break\n",
        "    # choose the q_max\n",
        "    q_max = q[state].max()\n",
        "\n",
        "    q_max_action = []\n",
        "    for action in range(6):\n",
        "        if q[state, action] == q_max:\n",
        "            q_max_action.append(action)\n",
        "    next_state = q_max_action[random.randint(0, len(q_max_action) - 1)]\n",
        "    print(\"the robot goes to \" + str(next_state) + '.')\n",
        "    state = next_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p42l7I1IKzaT",
        "outputId": "57c26005-b40f-4604-a56a-06d5ac3c68ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the robot goes to0\n",
            "the robot goes to 4.\n",
            "the robot goes to 5.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is a simple implementation of a reinforcement learning algorithm, specifically the Q-learning algorithm. It is designed to learn the optimal policy for a given environment, which means it learns the best action to take in each state to maximize the expected cumulative reward.\n",
        "\n",
        "Here's a breakdown of the code:\n",
        "\n",
        "    state = random.randint(0, 5): This line selects a random state from 0 to 5. This is the initial state the agent starts in.\n",
        "    print('the robot goes to{}'.format(state)): This line prints a message indicating the current state.\n",
        "    count = 0: This line initializes a variable count to 0. This variable is used to keep track of the number of iterations of the loop.\n",
        "    while state != 5: This line starts a while loop that runs until the state is 5. This means the agent continues to explore the environment until it reaches the final state.\n",
        "    if count > 20:: This line checks if the variable count is greater than 20. If it is, the agent has explored the environment for long enough and it's time to stop.\n",
        "    print('fail'): This line prints a message indicating that the agent has failed to find the optimal policy.\n",
        "    break: This line exits the while loop.\n",
        "    q_max = q[state].max(): This line finds the maximum Q-value for the current state. The Q-value represents the expected return for taking a particular action in a particular state.\n",
        "    q_max_action = []: This line initializes an empty list called q_max_action that will store the actions with the maximum Q-value.\n",
        "    for action in range(6):: This line loops through all possible actions (0 to 5) for the current state.\n",
        "    if q[state, action] == q_max:: This line checks if the Q-value for the current state and action is equal to the maximum Q-value found in step 8. If it is, the action is added to the q_max_action list.\n",
        "    next_state = q_max_action[random.randint(0, len(q_max_action) - 1)]: This line selects a random action from the q_max_action list and sets it as the next state. This is a simple way to implement the exploration-exploitation trade-off. The agent explores new actions and states, but also exploits the current knowledge to maximize the reward.\n",
        "    `print(\"the robot goes to \" + str(next_state) + '.'): This line prints a message indicating the next state the agent will visit.\n",
        "    state = next_state: This line sets the current state to the next state.\n",
        "\n",
        "Overall, this code implements a simple Q-learning algorithm that learns the optimal policy for a given environment. The agent explores the environment, updates the Q-values based on the rewards received, and uses the Q-values to determine the best action to take in each state. The algorithm runs until the agent reaches the final state or the maximum number of iterations is reached."
      ],
      "metadata": {
        "id": "TOfPhLWGRSEG"
      }
    }
  ]
}