# Simple Q-learning Treasure Hunt in R

# Set random seed for reproducibility
set.seed(123)

# Define environment
n_states <- 5          # 5 positions in a straight line
n_actions <- 2         # Left (1), Right (2)
treasure <- 5          # Treasure at position 5
trap <- 3              # Trap at position 3

# Parameters
alpha <- 0.1           # Learning rate
gamma <- 0.9           # Discount factor
epsilon <- 0.3         # Exploration rate
n_episodes <- 100      # Number of episodes

# Initialize Q-table
q_table <- matrix(0, nrow = n_states, ncol = n_actions)

# Choose action using epsilon-greedy policy
choose_action <- function(state) {
  if (runif(1) < epsilon) {
    return(sample(1:n_actions, 1))
  } else {
    return(which.max(q_table[state, ]))
  }
}

# Get next state
get_next_state <- function(state, action) {
  if (action == 1 && state > 1) {  # Left
    return(state - 1)
  } else if (action == 2 && state < n_states) {  # Right
    return(state + 1)
  }
  return(state)  # Stay if at boundary
}

# Get reward
get_reward <- function(state) {
  if (state == treasure) {
    return(10)    # Reward for finding treasure
  } else if (state == trap) {
    return(-5)    # Penalty for hitting trap
  } else {
    return(-1)    # Small penalty to encourage movement
  }
}

# Q-learning algorithm
for (episode in 1:n_episodes) {
  state <- 1  # Start at position 1
  
  while (state != treasure) {  # Until treasure is found
    # Choose action
    action <- choose_action(state)
    
    # Get next state and reward
    next_state <- get_next_state(state, action)
    reward <- get_reward(next_state)
    
    # Update Q-value
    current_q <- q_table[state, action]
    max_future_q <- max(q_table[next_state, ])
    new_q <- current_q + alpha * (reward + gamma * max_future_q - current_q)
    q_table[state, action] <- new_q
    
    # Move to next state
    state <- next_state
  }
  
  # Decay epsilon
  epsilon <- max(0.01, epsilon * 0.99)
}

# Function to show the learned path
show_path <- function() {
  path <- c(1)
  state <- 1
  world <- c("[S]", "[ ]", "[T]", "[ ]", "[G]")  # S=start, T=trap, G=goal
  
  while (state != treasure) {
    action <- which.max(q_table[state, ])
    next_state <- get_next_state(state, action)
    path <- c(path, next_state)
    state <- next_state
  }
  
  # Print the world and path
  cat("Treasure Hunt World:", paste(world, collapse = " "), "\n")
  cat("Learned Path:", paste(path, collapse = " -> "), "\n")
  
  # Print simple visualization of movement
  moves <- sapply(2:length(path), function(i) {
    if (path[i] > path[i-1]) "Right" else "Left"
  })
  cat("Moves:", paste(moves, collapse = ", "), "\n")
}

# Display results
cat("Final Q-table (Left, Right):\n")
print(round(q_table, 2))
show_path()


